
# Disaster Response Classification Pipeline

## Motivation

The feedback and report of disasters is a critical topic nowadays. By classifing various categories of fatal emergencies correctly, the authority could allocate resources timely and more efficiently so that people who are suffering cab be supported. The expectation of the project is to properly identify the key-in text as the true disaster.
By manipulating skills of Nature Language Process and establishing models of Machine Learning on real events, I am looking forward to acquiring experiences and practical techniques that can ben applied in the future when going through this project.

## Installation

1. Python3
2. Machine Learning Libraries: NumPy, Pandas, Scikit-Learn
3. Natural Language Process Libraries: nltk
4. SQLlite Database : sqlite3
5. Model Loading and Saving: Pickle
6. Web App and Data Visualization: Flask, Plotly

Please refer to 'requirement.txt' for details

## File Discription

Sharing by Figure 8, the datasets include two main sections, messages and categories, which contain the original text in foreign languages, messages translated in English, genre, and columns of categories. There are over 26,000 messages and 40 variables in the combined dataset. The brief discription of files is listed below.

Datasets:
1. data/disaster_messages.csv -> Dataframe includes columns of ID, translated messages, original messages, and genre
2. data/disaster_categories.csv --> Dataframe includes columns of ID and categories

Folder:
1. ETL Pipeline Preparation.ipynb --> Load and merge the datasets, remove unnecessary columns and rows, load the cleaned dataset as SQLite database file.
2. ML Pipeline Preparation.ipynb --> Load dataset from SQLite database, text tokenization and ML pipline establishment, split data into train and test set, train the models on Random Forest, Logistic Regression and SVM, tune the model by GridSearch, generate the result of prediction on test set and export the final model as a pickle file.

Python script:
1. data/process_data.py --> ETL pipeline
2. models/train_classifier.py - ML Pipeline
3. app/run.py - Flask Web App

Other: 
1. data/DisasterResponse.db --> SQLite database generated by ETL pipeline preparation
2. models/classifier.pkl --> pickle file generated by ML pipeline preparation

## Instructions

Run the following commands in the project's root directory to set up your database and model.

1. To run ETL pipeline that cleans data and stores in database python data/process_data.py data/disaster_messages.csv data/disaster_categories.csv data/DisasterResponse.db
2. To run ML pipeline that trains classifier and saves python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl
3. Run the following command in the app's directory to run your web app. python run.py

The project includes a web application that classifiy categories of disasters. User can type-in messages and the web application would identify types of disaters automatically.


## Licensing and Acknowledgements
1. The datasets are shared by Udacity from [Figure Eight] (https://appen.com/#customers), an Append Company
2. Most sections of Flask Web App is shared by [Udacity] (https://www.udacity.com/)

## Screenshots

Screenshot of top 10 categoreis
![image](https://github.com/ezman1105/Data-Scientist----DisasterResponsePipeline/tree/f214c2c840186cf73c5ba80bfabfe294765079cd/Images/top10categories.png)

Screenshot of Distribution of "Aid-related" with genre
![image](https://github.com/ezman1105/Data-Scientist----DisasterResponsePipeline/blob/f214c2c840186cf73c5ba80bfabfe294765079cd/Images/aidrelated.png)


